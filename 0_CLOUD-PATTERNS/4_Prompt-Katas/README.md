# Cloud Prompt Katas

Concept: Use cloud tasks as a basis for testing quality of output of GenAI tool prompts. 

<img src="https://github.com/lynnlangit/learning-cloud/blob/master/images/prompt-kata-group.png" width=800>

## Concepts / Test Kata Problems
- derive from tool testing
- derive from problem/tasks
- derive from 'found' prompts

### GenAI / LLM Exploration Concepts
Katas for practice/play/learning w/Prompts as 'code'
- Iteration on exact prompt langauge as 'params'
- Attention to tool-based prompt guidance/automated prompt re-writing
- Big (general purpose) / small (domain-specific) LLM comparisons
- Tool (GUI) vs API calls
- Improvements via examining output from 'thinking' LLM models

### Validation 
- HITL - domain expertise as validation
- Advsersarial 
    - different models settings within tool
    - different models within tool
    - different tools
- Automated
    - methods of automated, quantified output quality validation

## List of Candidate GenAI Tools

### General Purpose
- ChatGPT / OpenAI
- Google Gemini
- Claude / Anthropic

### Domain-specific 
- Code: GitHub CoPilot & Copilot Repositories
- Code: Repl.it
- Cloud: GCP Gemini
- Cloud: Amazon Q
- Images: Google Imagen3

- Medical Images: BioMedCLIP (open source foundational LLM)
- Video: Google Veo2
- Data: Databricks Genie, DBRX

### New Search Engines
- Perplexity (multiple models)

### Ethical 'no's
- DeepSeek
- Llama
